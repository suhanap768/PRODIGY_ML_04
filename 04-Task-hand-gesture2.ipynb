{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29b12cd5-cc7a-43db-9c06-2d4b704779c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing gestures: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:13<00:00,  1.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved label classes to 'label_classes.npy'\n",
      "âœ… Preprocessing complete!\n",
      "X_train shape: (9360, 64, 64, 1)\n",
      "y_train shape: (9360,)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Set path to your dataset\n",
    "TRAIN_DIR = \"train\"  # Adjust if your path is different\n",
    "IMG_SIZE = 64\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "# Loop through each gesture folder inside 'train'\n",
    "for gesture_folder in tqdm(os.listdir(TRAIN_DIR), desc=\"Processing gestures\"):\n",
    "    gesture_path = os.path.join(TRAIN_DIR, gesture_folder)\n",
    "\n",
    "    if not os.path.isdir(gesture_path):\n",
    "        continue\n",
    "\n",
    "    for image_file in os.listdir(gesture_path):\n",
    "        if not image_file.lower().endswith(\".jpg\"):\n",
    "            continue\n",
    "\n",
    "        image_path = os.path.join(gesture_path, image_file)\n",
    "        img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        if img is None:\n",
    "            continue\n",
    "\n",
    "        img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n",
    "        img = img / 255.0  # normalize to 0â€“1 range\n",
    "\n",
    "        X.append(img)\n",
    "        y.append(gesture_folder)\n",
    "\n",
    "# Convert to NumPy arrays\n",
    "X = np.array(X).reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n",
    "y = np.array(y)\n",
    "\n",
    "# Encode class names to numbers\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "np.save(\"label_classes.npy\", le.classes_)  # Save gesture names\n",
    "print(\"âœ… Saved label classes to 'label_classes.npy'\")\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_encoded, test_size=0.2, stratify=y_encoded, random_state=42\n",
    ")\n",
    "\n",
    "# One-hot encode labels for softmax\n",
    "y_train_cat = to_categorical(y_train)\n",
    "y_test_cat = to_categorical(y_test)\n",
    "\n",
    "print(\"âœ… Preprocessing complete!\")\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "109e5721-1e47-49f9-a2c9-db47a6349c6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m293/293\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 103ms/step - accuracy: 0.8938 - loss: 0.3748 - val_accuracy: 1.0000 - val_loss: 1.4504e-05\n",
      "Epoch 2/15\n",
      "\u001b[1m293/293\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 89ms/step - accuracy: 0.9974 - loss: 0.0083 - val_accuracy: 1.0000 - val_loss: 1.4738e-07\n",
      "Epoch 3/15\n",
      "\u001b[1m293/293\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 97ms/step - accuracy: 0.9988 - loss: 0.0044 - val_accuracy: 1.0000 - val_loss: 4.4393e-07\n",
      "Epoch 4/15\n",
      "\u001b[1m293/293\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 82ms/step - accuracy: 0.9996 - loss: 0.0023 - val_accuracy: 1.0000 - val_loss: 1.0733e-07\n",
      "Epoch 5/15\n",
      "\u001b[1m293/293\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 94ms/step - accuracy: 0.9982 - loss: 0.0041 - val_accuracy: 1.0000 - val_loss: 6.1133e-09\n",
      "Epoch 6/15\n",
      "\u001b[1m293/293\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 141ms/step - accuracy: 0.9996 - loss: 0.0013 - val_accuracy: 1.0000 - val_loss: 1.0189e-10\n",
      "Epoch 7/15\n",
      "\u001b[1m293/293\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 183ms/step - accuracy: 0.9999 - loss: 3.6842e-04 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 8/15\n",
      "\u001b[1m293/293\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 125ms/step - accuracy: 0.9999 - loss: 4.1878e-04 - val_accuracy: 1.0000 - val_loss: 4.0092e-08\n",
      "Epoch 9/15\n",
      "\u001b[1m293/293\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 110ms/step - accuracy: 0.9995 - loss: 0.0021 - val_accuracy: 1.0000 - val_loss: 2.2415e-09\n",
      "Epoch 10/15\n",
      "\u001b[1m293/293\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 73ms/step - accuracy: 1.0000 - loss: 4.3270e-04 - val_accuracy: 1.0000 - val_loss: 7.1322e-10\n",
      "\u001b[1m74/74\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Test Accuracy: 100.00%\n",
      "Model saved as 'gesture_new_model.keras'\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Input\n",
    "from keras.saving import save_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Build the CNN Model\n",
    "model = Sequential([\n",
    "    Input(shape=(IMG_SIZE, IMG_SIZE, 1)),\n",
    "    \n",
    "    Conv2D(32, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    \n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    \n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    \n",
    "    Dense(len(np.unique(y_encoded)), activation='softmax')  # 13 gestures\n",
    "])\n",
    "\n",
    "# Compile\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Optional: Stop early if not improving\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Train\n",
    "history = model.fit(\n",
    "    X_train, y_train_cat,\n",
    "    epochs=15,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_test, y_test_cat),\n",
    "    callbacks=[early_stop]\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test_cat)\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n",
    "\n",
    "# Save\n",
    "save_model(model, \"gesture_new_model.keras\")\n",
    "print(\"Model saved as 'gesture_new_model.keras'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ade65b0e-a83c-4014-8cdf-6a49f64d977d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Webcam started... Press 'q' to quit.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import time\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load model and labels\n",
    "IMG_SIZE = 64\n",
    "model = load_model(\"gesture_new_model.keras\")\n",
    "\n",
    "le = LabelEncoder()\n",
    "le.classes_ = np.load(\"label_classes.npy\")\n",
    "\n",
    "# Start webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "print(\"Webcam started... Press 'q' to quit.\")\n",
    "time.sleep(2)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Define Region of Interest (ROI)\n",
    "    x1, y1, x2, y2 = 200, 100, 400, 300\n",
    "    roi = frame[y1:y2, x1:x2]\n",
    "\n",
    "    # Preprocess ROI\n",
    "    gray = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)\n",
    "    resized = cv2.resize(gray, (IMG_SIZE, IMG_SIZE))\n",
    "    normalized = resized / 255.0\n",
    "    reshaped = normalized.reshape(1, IMG_SIZE, IMG_SIZE, 1)\n",
    "\n",
    "    # Predict gesture\n",
    "    pred = model.predict(reshaped, verbose=0)\n",
    "    predicted_class = np.argmax(pred)\n",
    "    gesture_name = le.inverse_transform([predicted_class])[0]\n",
    "\n",
    "    # Draw results\n",
    "    cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "    cv2.putText(frame, f\"Gesture: {gesture_name}\", (x1, y1 - 10),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)\n",
    "\n",
    "    cv2.imshow(\"Gesture Recognition\", frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96299008-d3a7-4325-9793-d46901500e35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“· Webcam started... Press 'q' to quit.\n",
      "Webcam closed.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import time\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load trained model and label encoder\n",
    "model = load_model(\"gesture_cnn_model.keras\")\n",
    "le = LabelEncoder()\n",
    "le.classes_ = np.load(\"label_classes.npy\")\n",
    "\n",
    "IMG_SIZE = 64  # Must match training image size\n",
    "\n",
    "# Initialize MediaPipe Hand detector\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(static_image_mode=False,\n",
    "                       max_num_hands=1,\n",
    "                       min_detection_confidence=0.7)\n",
    "mp_draw = mp.solutions.drawing_utils\n",
    "\n",
    "# Start webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "print(\"ðŸ“· Webcam started... Press 'q' to quit.\")\n",
    "time.sleep(2)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    h, w, _ = frame.shape\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    result = hands.process(frame_rgb)\n",
    "\n",
    "    if result.multi_hand_landmarks:\n",
    "        for hand_landmarks in result.multi_hand_landmarks:\n",
    "            # Get bounding box coordinates\n",
    "            x_coords = [lm.x for lm in hand_landmarks.landmark]\n",
    "            y_coords = [lm.y for lm in hand_landmarks.landmark]\n",
    "            x_min = int(min(x_coords) * w) - 15\n",
    "            x_max = int(max(x_coords) * w) + 15\n",
    "            y_min = int(min(y_coords) * h) - 15\n",
    "            y_max = int(max(y_coords) * h) + 15\n",
    "\n",
    "            # Clamp coordinates within frame\n",
    "            x_min, y_min = max(0, x_min), max(0, y_min)\n",
    "            x_max, y_max = min(w, x_max), min(h, y_max)\n",
    "\n",
    "            roi = frame[y_min:y_max, x_min:x_max]\n",
    "            if roi.size == 0:\n",
    "                continue\n",
    "\n",
    "            # Preprocess ROI\n",
    "            gray = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)\n",
    "            resized = cv2.resize(gray, (IMG_SIZE, IMG_SIZE))\n",
    "            normalized = resized / 255.0\n",
    "            reshaped = normalized.reshape(1, IMG_SIZE, IMG_SIZE, 1)\n",
    "\n",
    "            # Predict\n",
    "            pred = model.predict(reshaped, verbose=0)\n",
    "            predicted_class = np.argmax(pred)\n",
    "            confidence = np.max(pred)\n",
    "\n",
    "            # Display prediction if confident enough\n",
    "            gesture_name = le.inverse_transform([predicted_class])[0]\n",
    "\n",
    "            # Draw box and label\n",
    "            cv2.rectangle(frame, (x_min, y_min), (x_max, y_max), (0, 255, 0), 2)\n",
    "            cv2.putText(frame, f\"{gesture_name} ({confidence*100:.1f}%)\", (x_min, y_min - 10),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 255, 255), 2)\n",
    "\n",
    "            # Draw hand landmarks\n",
    "            mp_draw.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "    # Show webcam frame\n",
    "    cv2.imshow(\"Real-Time Gesture Recognition\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Clean up\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "print(\"Webcam closed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b43df31-86b8-4fae-995d-b6240717cb30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mediapipe_env)",
   "language": "python",
   "name": "mediapipe_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
